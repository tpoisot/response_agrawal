% Don't be afraid of Open Access
% List of authors to be determined
% ...

In a recent Letter, @agrawal raises four points for which scientists should
be skeptical of the Open Access (OA) movement, including the fact that
publishing in OA journals can *lower* one's impact. Before we present a
dissenting view, we feel like clarifying some mis-conceptions is in order. OA
is a mode of diffusion of scientific literature in which the authors, or
its home institution, buys back the rights of an article to the publisher,
so that the article is free to access. It is generally agreed that the
article is free, to use a metaphor from software development, "as in beer"
(*i.e.* there is no need to pay for it), and "as in speech" (*i.e.* there
are minimal limits to its reproduction, re-use, and possibly modification).

That some "predatory" publishers are taking advantage of this model
to generate a profit is undeniable. However, these are well identified,
and easy to avoid. There is a list of such publishers ([link to list])
available on line for easy reference. In itself, OA is not built on whether
a profit will be made (many non-OA journals, for example those operated
by societies, are for-profit), but on the making the output of research
available to the largest possible number. The fact that OA journals are not
necessarily *non-selective* has been covered in the response by Lanford &
Pennel's response, so we will not elaborate on this point any further.

@agrawal raises two points against OA that are of particular relevance for
early career scientists. First, the impact factor of OA journals is not higher
than the impact factor of non-OA journals. Second, it is tempting to use the
journal as a surrogate for the quality of a paper, and less prestigious OA
journals run a risk of seeing good papers viewed less favorably. We do not
think that any of these points are valid arguments against open access.

First, the distinction between OA and non-OA journals is a false
dichotomy. Although there are journals applying an open access (often of the
*Creative Commons* variety) to all articles they publish, an increasing number
of publishers allow authors to pay a *per* article OA license to retain their
copyright, and so there exists a continuum between pure-OA, mixed-models,
and closed journals. Even though, the notion that pure-OA journals have
a lower impact is challenged by some. James Pringle of Thomson ISI (i)
recognizes that the relevance of journals when talking about OA is dubious
and (ii) "prospective authors should not fear publishing in these journals"
[http://www.nature.com/nature/focus/accessdebate/19.html].

Second, the fact that OA journals carry less prestige is not a problem with
the OA movement. Measures of journal impact are known to be extremely biased
by a few papers concentrating a few citations, and cannot possibly be used to
estimate the quality of a paper, let alone its *likely impact*. Simply put,
the impact factor of a journal is not the expected number of citations a single
paper will receive. If rigor is important to us, then it is unacceptable to
think that a paper is bad because it was published in a less respectable
journal, or that a paper is good because it was published in a highly
selective journal; the same paper is just as good whether published in *PLoS
One* or *Nature*. This point strikes us as a demonstration that the metrics
used for evaluations are biased. The recent years say the development of
*article-level* metrics, which are able to measure the impact of an article
regardless of the journal it appeared in. We think that rather than pushing
against the OA model, we should have a discussion about how these measures
can be used in the evaluation of scientists.

@agrawal concludes his paper on the need to find *an alternative model of
publishing that suits the primary goals of scientists*. On that, we could not
agree more. However, what that *primary goal* is seems open to debate. We
would like to make the point that, particularly in ecology and environmental
sciences, the primary goal of research should be to produce fundamental insights
that can be mobilized to solve large scale problems. Making information flow
freely between scientists, policy-makers, and stakeholders is paramount to
this effort. What does not strikes us as a *primary* goal, is the maximization
of self-aggrandizing, not to mention arbitrary, measures of impact.
